{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "026b8c71",
   "metadata": {},
   "source": [
    "# **INTRODCTION**\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27ae818",
   "metadata": {},
   "source": [
    "### **1. LIBRARIES**\n",
    "- **Installing dependencies:** using `pip`, install `python-dotenv`,\n",
    "-  **How to get an Open AI API Key:** <a href=\"https://platform.openai.com/\">Open AI Platform</a> \n",
    "-  **How to store your API keys SAFELY:** `.env`, `.gitignore`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d716b6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in c:\\users\\tchan\\anaconda3\\envs\\data-cleaning-agent\\lib\\site-packages (1.1.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Installing dependencies\n",
    "%pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "806eba4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Dependencies\n",
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119828bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_dotenv(), os.envrion, os.environ.get()\n",
    "load_dotenv()\n",
    "\n",
    "# Getting\n",
    "os.environ['OPENAI_API_KEY']\n",
    "# If not found it will error try add an A\n",
    "os.environ['KEY_DONT_EXIST']\n",
    "\n",
    "# By using .get() you can add a fallback value\n",
    "os.environ.get(\"OPENAI_API_KEY\", None)\n",
    "\n",
    "# Can help deal with errors:\n",
    "KEY = os.environ.get('KEY_DONT_EXIST', None)\n",
    "if not KEY:\n",
    "    print(\"KEY NOT FOUND\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60fd822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign OPENAI_API_KEY \n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78c2f76",
   "metadata": {},
   "source": [
    "### **2. OpenAI Library**\n",
    "- **OpenAI Response API:** Refer to <a href=\"https://platform.openai.com/docs/api-reference/responses/create?lang=python\">OpenAI Responses Docs</a>\n",
    "- **OpenAI ChatCompletions API:** Refer to <a href=\"https://platform.openai.com/docs/api-reference/chat/create?lang=python\">OpenAI Chat Completions Docs</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a4f166d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install openai\n",
    "%pip install openai\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd9677a",
   "metadata": {},
   "source": [
    "### Initialising OpenAI Client and Using Responses API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8ce50393",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Response(id='resp_68beee9c1f1481979a217d453732328c09be5d29cc5dfc3e', created_at=1757343388.0, error=None, incomplete_details=None, instructions=None, metadata={}, model='gpt-4o-mini-2024-07-18', object='response', output=[ResponseOutputMessage(id='msg_68beee9d14b08197b51275e7cef55b9609be5d29cc5dfc3e', content=[ResponseOutputText(annotations=[], text=\"That's great to hear! What are you currently studying or working on in computer science? Do you have any specific topics or projects you’d like to discuss?\", type='output_text', logprobs=[])], role='assistant', status='completed', type='message')], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[], top_p=1.0, background=False, conversation=None, max_output_tokens=None, max_tool_calls=None, previous_response_id=None, prompt=None, prompt_cache_key=None, reasoning=Reasoning(effort=None, generate_summary=None, summary=None), safety_identifier=None, service_tier='default', status='completed', text=ResponseTextConfig(format=ResponseFormatText(type='text'), verbosity='medium'), top_logprobs=0, truncation='disabled', usage=ResponseUsage(input_tokens=14, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=32, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=46), user=None, store=True)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Call openai with your API key\n",
    "client = OpenAI(api_key=OPENAI_API_KEY) # convention is to call it client\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-4o-mini\", # Try using restricted model to show restrictions\n",
    "    input=\"Hi I'm a computer science student!\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d1dfa29a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response(id='resp_68beee9c1f1481979a217d453732328c09be5d29cc5dfc3e', created_at=1757343388.0, error=None, incomplete_details=None, instructions=None, metadata={}, model='gpt-4o-mini-2024-07-18', object='response', output=[ResponseOutputMessage(id='msg_68beee9d14b08197b51275e7cef55b9609be5d29cc5dfc3e', content=[ResponseOutputText(annotations=[], text=\"That's great to hear! What are you currently studying or working on in computer science? Do you have any specific topics or projects you’d like to discuss?\", type='output_text', logprobs=[])], role='assistant', status='completed', type='message')], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[], top_p=1.0, background=False, conversation=None, max_output_tokens=None, max_tool_calls=None, previous_response_id=None, prompt=None, prompt_cache_key=None, reasoning=Reasoning(effort=None, generate_summary=None, summary=None), safety_identifier=None, service_tier='default', status='completed', text=ResponseTextConfig(format=ResponseFormatText(type='text'), verbosity='medium'), top_logprobs=0, truncation='disabled', usage=ResponseUsage(input_tokens=14, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=32, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=46), user=None, store=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"That's great to hear! What are you currently studying or working on in computer science? Do you have any specific topics or projects you’d like to discuss?\""
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show response object\n",
    "print(response)\n",
    "# To extract content from response *Go through step by step\n",
    "response.output[0].content[0].text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd33908",
   "metadata": {},
   "source": [
    "#### Introducing ChatCompletions (What were using)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f93a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChatCompletions allows for control over chain of messages also simpler\n",
    "\n",
    "# Message object roles: \"system\", \"user\", \"assistant\"\n",
    "messages = []\n",
    "messages.append({\"role\": \"system\", \"content\": \"You are a helpful chat assistant\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec685612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a user query\n",
    "query = \"Hi I'm a computer science student!\"\n",
    "messages.append({\"role\":\"user\", \"content\": query})\n",
    "\n",
    "# How to call chat completions\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\", # Try using restricted model to show restrictions\n",
    "    messages=messages\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "94651550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-CDXrn9z2RI0qYZxe4tqy0D3UiQfta', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"Hi there! That's great to hear! What specific areas of computer science are you interested in, or do you have any questions or topics you'd like to discuss?\", refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1757344483, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier='default', system_fingerprint='fp_e665f7564b', usage=CompletionUsage(completion_tokens=32, prompt_tokens=24, total_tokens=56, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Hi there! That's great to hear! What specific areas of computer science are you interested in, or do you have any questions or topics you'd like to discuss?\""
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect ChatCompletion object\n",
    "print(response)\n",
    "\n",
    "# ChatCompletion Object Text Extraction\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4387f2",
   "metadata": {},
   "source": [
    "### **3. CALLING LANGCHAIN**\n",
    "- **Langchain OpenAI**: `llm = ChatOpenAI()`\n",
    "- **Message Types:** `SystemMessage`, `HumanMessage`, `AIMessage`, Message chaining (mention later)\n",
    "- **Invoking LLM**: `llm.invoke(messages)`\n",
    "- **Prompting the AI:** Prompt engineering, giving AI instructions!\n",
    "- **Langchain-OpenAI:** Documentation: <a href=\"https://python.langchain.com/docs/integrations/chat/openai/\">Langchain Docs</a>\n",
    "- **LangSmith Tracing *(Optional)*** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97a96de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install and import langchain\n",
    "%pip install langchain langchain_openai\n",
    "from langchain_openai import ChatOpenAI # import ChatCompletions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af6b98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intialise our \"llm\" (ChatOpenAI) no API Key needed, auto reads\n",
    "\"\"\"\n",
    "What is Temperature?\n",
    "Low temperature (e.g., 0 or 0.1):\n",
    "- The model is more deterministic and focused. \n",
    "- It will give more predictable, safe, and repetitive answers.\n",
    "- *What we want for data cleaning agent*\n",
    "High temperature (e.g., 0.7 or 1):\n",
    "- The model is more creative and random. \n",
    "- It may generate more diverse or unexpected responses.\n",
    "- Higher temperatures: Creative marketing content, storytelling, chatbots\n",
    "\"\"\"\n",
    "llm = ChatOpenAI(temperature=0, model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0e2a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import HumanMessage, SystemMessage # Import Message schemas\n",
    "\n",
    "# Creating messages with Langchain's message schemas\n",
    "messages=[]\n",
    "# System Message (AI prompt/instructions)\n",
    "messages.append(SystemMessage(content=\"You are a helpful chat assistant\"))\n",
    "# User Query\n",
    "query = \"Hi I'm a computer science student!\"\n",
    "messages.append(HumanMessage(content=query))\n",
    "\n",
    "# inspect messages\n",
    "print(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33923b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoking LLM with messages\n",
    "response = llm.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b33afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect Response\n",
    "print(response)\n",
    "\n",
    "# Extracting Response Text\n",
    "response.content # Much simpler!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a09c80",
   "metadata": {},
   "source": [
    "### **4. RUNNING CODE WITH AI**\n",
    "- **How to Prompt Engineer:** Prompt the AI to generate datacleaning code\n",
    "- **`exec()` Function:** Running the code, assume existing variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5d3bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install and import pandas, numpy\n",
    "%pip install pandas numpy\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "2e9fe21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import in a sample df (Life Expectancy Data.csv)\n",
    "df = pd.read_csv(\"../datasets/Life Expectancy Data.csv\")\n",
    "\n",
    "# Intialise llm\n",
    "llm = ChatOpenAI(temperature=0, model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd084c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages=[]\n",
    "\n",
    "# Create a datacleaning agent prompt (Explain each rule each line)\n",
    "prompt = \"\"\" You are a data cleaning agent\n",
    "\n",
    "Help generate pandas code to deal with user data cleaning queries.\n",
    "Rules:\n",
    "- Return only executable Python code, no explanations, no markdown blocks\n",
    "- Assume dataframe is stored in variable 'df', modify it INPLACE\n",
    "- In order for user to see your message you have to print your commands/messages\n",
    "\"\"\"\n",
    "messages.append(SystemMessage(content=prompt))\n",
    "\n",
    "# Give the AI context (dataset info)\n",
    "dataset_info = f\"Dataset Info: {df.shape} Sample Data: {df.head(5)}\"\n",
    "messages.append(SystemMessage(content=dataset_info))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "dfb20b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# User query\n",
    "\"\"\"\n",
    "Sample Queries:\n",
    "- how many missing values in dataset? (Easy, minimal prompt engineering)\n",
    "- impute misssing values in dataset? (May error, modifying df, introduce rules)\n",
    "\"\"\"\n",
    "\n",
    "query=\"how many missing values in dataset?\"\n",
    "messages.append(HumanMessage(content=query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53ac4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call LLM and save output to responses\n",
    "response = llm.invoke(messages)\n",
    "\n",
    "# Save output content as generated_code\n",
    "generated_code = response.content\n",
    "print(generated_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f507cac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run generated code\n",
    "exec(generated_code)\n",
    "\n",
    "# Run with exception handling to avoid errors stopping program\n",
    "try:\n",
    "    original_df = df.copy() # store a copy in case\n",
    "    exec(generated_code)\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    print(f\"Generated Code: {generated_code}\")\n",
    "    df = original_df # reverting df in function we would (return original_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f84670",
   "metadata": {},
   "source": [
    "### **5. CREATING HELPER FUNCTIONS**\n",
    "- **Running Helper Functions:** Creating \"helper docs\"\n",
    "- **Importing Functions:** How to import functions from other notebooks using `import-ipynb`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "222358d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import in a sample df (Life Expectancy Data.csv)\n",
    "df = pd.read_csv(\"../datasets/Life Expectancy Data.csv\")\n",
    "\n",
    "# Creating a helper function to assist AI\n",
    "def impute_with_mean(series):\n",
    "    series.fillna(series.mean(), inplace=True)\n",
    "    return series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "37ff6f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages=[]\n",
    "\n",
    "# Copy Paste previous prompt\n",
    "prompt = \"\"\" You are a data cleaning agent\n",
    "\n",
    "Help generate pandas code to deal with user data cleaning queries.\n",
    "Rules:\n",
    "- Return only executable Python code, no explanations, no markdown blocks\n",
    "- Assume dataframe is stored in variable 'df', modify it INPLACE\n",
    "- In order for user to see your message you have to print your commands/messages\n",
    "\"\"\"\n",
    "messages.append(SystemMessage(content=prompt))\n",
    "\n",
    "# Give the AI dataset info\n",
    "dataset_info = f\"Dataset Info: {df.shape} Sample Data: {df.head(5)}\"\n",
    "messages.append(SystemMessage(content=dataset_info))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "259d3c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Docs (introduces AI to your functions)\n",
    "helper_docs = \"\"\" HELPER FUNCTIONS AVAILABLE:\n",
    "impute_with_mean(series)\n",
    "- Takes a pandas series and imputes with mean\n",
    "- Returns imputed series\n",
    "\"\"\" \n",
    "messages.append(SystemMessage(content=helper_docs))\n",
    "\n",
    "# Sample query to test out function\n",
    "query=\"impute numeric columns with mean\"\n",
    "messages.append(HumanMessage(content=query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c99dcb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call LLM and save output to responses\n",
    "response = llm.invoke(messages)\n",
    "\n",
    "# Save output content as generated_code\n",
    "generated_code = response.content\n",
    "print(generated_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d267197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try running the generated code with helper docs\n",
    "try:\n",
    "    original_df = df.copy()\n",
    "    exec(generated_code)\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    print(f\"Generated Code: {generated_code}\")\n",
    "    df = original_df\n",
    "\n",
    "# Check number of means:\n",
    "print(df.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5e40eb",
   "metadata": {},
   "source": [
    "### **OTHER POINTS**\n",
    "- Message Chaining\n",
    "- Langchain Agents with Tool Calling\n",
    "- `.py` vs `.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1075343",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-cleaning-agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
